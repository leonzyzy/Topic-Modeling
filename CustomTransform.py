### Short Description for Project Proposal  

Training machine learning models on terabyte-scale datasets presents significant challenges, including high memory usage, the need for substantial disk storage, and inefficiencies in data management and processing workflows. Existing distributed training frameworks often require users to download, preprocess, and manage large datasets locally, leading to considerable resource overheads and scalability limitations. These challenges become even more pronounced in multi-node, multi-GPU streaming distributed training setups, highlighting the need for an efficient and scalable solution to simplify large-scale training pipelines.  

In this project, we propose a novel end-to-end framework that leverages LitData, Spark ETL pipelines, Cloud (MLZone), and PyTorch to address these challenges. Spark ETL pipelines enable efficient data loading and preprocessing, while LitData scales data processing tasks such as distributed inference, embedding creation, and resizing across local and cloud environments. Additionally, LitData optimizes datasets to accelerate training and allows users to work with large remote datasets without requiring local storage, overcoming traditional memory and disk constraints. By integrating these components, we build a user-friendly framework that chunks large datasets, supports efficient streaming distributed training across multi-GPU and multi-node setups, and significantly reduces time complexity, space requirements, and operational overhead. This talk will detail our framework's design, its real-world performance, the challenges we tackled, and our vision for future improvements.
